from flask import Flask, request, jsonify, session
import openai
import pandas as pd
import textwrap
import builtins
import time
import json
import requests
import os
import re
from typing import List, Tuple
from pydantic import BaseModel, field_validator, ValidationError, ValidationInfo
from typing import List, Optional
from fuzzywuzzy import process, fuzz
from flask_session import Session
from flask_cors import CORS
import ast
from agent_graph.graph import create_graph, compile_workflow
from utils.helper_functions import load_config
from models.openai_models import get_open_ai
import pickle
from datetime import datetime
from utils.helper_functions import load_config  # Ensure this is imported
from textblob import TextBlob
import folium
from folium.plugins import MarkerCluster
import numpy as np


app = Flask(__name__)

CORS(app)

load_config('config/config.yaml')  # Specify the correct path if different

def load_collection_data_frames(filename='collection_data_frames.pkl'):
    """
    Loads the pickled DataFrame from the specified file.

    :param filename: Path to the pickle file.
    :return: Loaded Pandas DataFrame.
    """
    try:
        with open(filename, 'rb') as file:
            data_frames = pickle.load(file)
        print(f"Data successfully loaded from {filename}")
        return data_frames
    except Exception as e:
        print(f"Error loading pickle file: {e}")
        raise e
    
location_data_frames = load_collection_data_frames("location_data_frames.pkl")

#Generated by ChatGPT
def load_collection_data_frames(filename='collection_data_frames.pkl'):
    """
    Loads the pickled DataFrame from the specified file.

    :param filename: Path to the pickle file.
    :return: Loaded Pandas DataFrame.
    """
    try:
        with open(filename, 'rb') as file:
            data_frames = pickle.load(file)
        print(f"Data successfully loaded from {filename}")
        return data_frames
    except Exception as e:
        print(f"Error loading pickle file: {e}")
        raise e

def parse_condition(cond_str, series):
    """
    Parses a condition string and returns a boolean mask that applies the condition to a Series.

    Supported condition formats:
    - "x<value"
    - "x<=value"
    - "x>value"
    - "x>=value"
    - "x==value"
    - "x!=value"
    - "value1<x<value2"
    - "value1<=x<=value2"
    - "value1<x<=value2"
    - "value1<=x<value2"

    :param cond_str: Condition string (e.g., "0<x<50.4", "x<25").
    :param series: Pandas Series to apply the condition on.
    :return: A Pandas Series representing the condition mask.
    """
    # Remove any spaces
    cond_str = cond_str.replace(" ", "")

    # Patterns for different condition types
    pattern_between = re.compile(r'^([\d\.]+)([<>]=?)x([<>]=?)([\d\.]+)$')
    pattern_single = re.compile(r'^(x)([<>]=?)([\d\.]+)$')
    pattern_single_rev = re.compile(r'^([\d\.]+)([<>]=?)(x)$')

    # Initialize mask as None
    mask = None

    # Check for between conditions like "0<x<50.4" or "10<=x<=20"
    match_between = pattern_between.match(cond_str)
    if match_between:
        val1, op1, op2, val2 = match_between.groups()
        val1 = float(val1)
        val2 = float(val2)

        # Determine lower and upper bounds based on operators
        if op1 in ['<', '<=']:
            lower = val1
            lower_inclusive = op1 == '<='
        else:
            lower = val2
            lower_inclusive = op2 == '<='

        if op2 in ['<', '<=']:
            upper = val2
            upper_inclusive = op2 == '<='
        else:
            upper = val1
            upper_inclusive = op1 == '<='

        # Apply the conditions
        if lower_inclusive:
            mask = series >= lower
        else:
            mask = series > lower

        if upper_inclusive:
            mask &= series <= upper
        else:
            mask &= series < upper

        return mask

    # Check for single conditions like "x<25", "x>=10"
    match_single = pattern_single.match(cond_str)
    if match_single:
        _, operator, value = match_single.groups()
        value = float(value)

        if operator == '<':
            mask = series < value
        elif operator == '<=':
            mask = series <= value
        elif operator == '>':
            mask = series > value
        elif operator == '>=':
            mask = series >= value
        elif operator == '==':
            mask = series == value
        elif operator == '!=':
            mask = series != value
        else:
            raise ValueError(f"Unsupported operator in condition: {cond_str}")

        return mask

    # Check for reversed single conditions like "10<x"
    match_single_rev = pattern_single_rev.match(cond_str)
    if match_single_rev:
        value, operator, _ = match_single_rev.groups()
        value = float(value)

        if operator == '<':
            mask = series > value
        elif operator == '<=':
            mask = series >= value
        elif operator == '>':
            mask = series < value
        elif operator == '>=':
            mask = series <= value
        elif operator == '==':
            mask = series == value
        elif operator == '!=':
            mask = series != value
        else:
            raise ValueError(f"Unsupported operator in condition: {cond_str}")

        return mask

    # If no pattern matches, raise an error
    raise ValueError(f"Unsupported condition format: {cond_str}")

def search_database(df, search_criteria):
    """
    Searches the DataFrame based on the provided search criteria.

    :param df: Pandas DataFrame to search.
    :param search_criteria: Dictionary containing search conditions and columns to return.
                            Example:
                            {
                                'location': 'Amsterdam',
                                'category': 'Accommodation',
                                'lat': '0<x<50.4',
                                'lng': 'x<25',
                                'columns': 'all'
                            }
    :return: Filtered Pandas DataFrame based on the search criteria.
    """
    mask = pd.Series([True] * len(df), index=df.index)

    # Iterate through each key in the search criteria
    for key, value in search_criteria.items():
        if key == 'columns':
            continue  # Handle columns later
        if key not in df.columns:
            print(f"Warning: Column '{key}' does not exist in the DataFrame and will be ignored.")
            continue

        if isinstance(value, str) and 'x' in value:
            try:
                condition_mask = parse_condition(value, df[key])
                mask &= condition_mask
            except ValueError as ve:
                print(f"Error parsing condition for column '{key}': {ve}")
                continue
        else:
            # Exact match
            mask &= df[key] == value

    # Apply the mask to filter the DataFrame
    try:
        filtered_df = df[mask]
    except Exception as e:
        print(f"Error applying mask: {e}")
        raise e

    # Handle the 'columns' key
    columns_to_return = search_criteria.get('columns', 'all')
    if columns_to_return != 'all':
        if isinstance(columns_to_return, list):
            # Validate columns
            valid_columns = [col for col in columns_to_return if col in df.columns]
            invalid_columns = set(columns_to_return) - set(valid_columns)
            if invalid_columns:
                print(f"Warning: The following columns are invalid and will be ignored: {invalid_columns}")
            if valid_columns:
                filtered_df = filtered_df[valid_columns]
            else:
                print("Warning: No valid columns specified. Returning all columns.")
        else:
            print("Warning: 'columns' should be a list or 'all'. Returning all columns.")

    return filtered_df

# Generated by ChatGPT
def reduce_dataframe(df: pd.DataFrame, search_criteria: dict) -> pd.DataFrame:
    """
    Reduces the size of a DataFrame based on specified search criteria.

    Parameters:
    - df (pd.DataFrame): The original DataFrame to be reduced.
    - search_criteria (dict): A dictionary containing search conditions and a 'columns' key.

    Returns:
    - pd.DataFrame: A reduced DataFrame with at most 10 rows based on the criteria.
    """

    # Copy the DataFrame to avoid modifying the original
    df_reduced = df.copy()

    # Helper function to parse condition strings
    def parse_condition(condition: str):
        """
        Parses a condition string and returns its components.

        Supported Formats:
        - "x < value"
        - "x <= value"
        - "x > value"
        - "x >= value"
        - "x == value"
        - "x != value"
        - "value1 < x < value2"
        - "value1 <= x <= value2"
        - "value1 < x <= value2"
        - "value1 <= x < value2"

        Returns:
        - dict: A dictionary with the type of condition and relevant values.
        """
        condition = condition.strip()

        # Handle 'x != value'
        if '!=' in condition:
            parts = condition.split('!=')
            value = float(parts[1].strip())
            return {'type': 'not_equal', 'value': value}

        # Handle range conditions like "value1 < x < value2"
        range_match = re.match(r'^(\d+(?:\.\d+)?)\s*[<>]=?\s*x\s*[<>]=?\s*(\d+(?:\.\d+)?)$', condition)
        if range_match:
            value1, value2 = range_match.groups()
            return {'type': 'range', 'value1': float(value1), 'value2': float(value2)}

        # Handle single conditions like "x > value"
        single_match = re.match(r'^x\s*([<>]=?)\s*(\d+(?:\.\d+)?)$', condition)
        if single_match:
            operator, value = single_match.groups()
            return {'type': 'single', 'operator': operator, 'value': float(value)}

        # If condition format is unrecognized
        return {'type': 'unknown'}

    # Helper function to compute distance for 'x != value'
    def compute_farthest(df: pd.DataFrame, column: str, value: float):
        """
        Computes the distance from a value and selects the top 10 farthest rows.

        Parameters:
        - df (pd.DataFrame): The DataFrame.
        - column (str): The column to compute distance on.
        - value (float): The value to compute distance from.

        Returns:
        - pd.DataFrame: Top 10 farthest rows based on the distance.
        """
        df['distance'] = abs(df[column] - value)
        df_sorted = df.sort_values(by='distance', ascending=False).head(10)
        df_sorted = df_sorted.drop(columns=['distance'])
        return df_sorted

    # Helper function to compute closest rows based on a value or range
    def compute_closest(df: pd.DataFrame, column: str, condition: dict):
        """
        Computes the distance based on the condition and selects the top 10 closest rows.

        Parameters:
        - df (pd.DataFrame): The DataFrame.
        - column (str): The column to compute distance on.
        - condition (dict): Parsed condition dictionary.

        Returns:
        - pd.DataFrame: Top 10 closest rows based on the distance.
        """
        if condition['type'] == 'single':
            operator = condition['operator']
            value = condition['value']

            if operator in ['>', '>=']:
                # Higher values are closer
                df_sorted = df.sort_values(by=column, ascending=False).head(10)
                return df_sorted
            elif operator in ['<', '<=']:
                # Lower values are closer
                df_sorted = df.sort_values(by=column, ascending=True).head(10)
                return df_sorted
            elif operator in ['==']:
                # Closest to the value
                df['distance'] = abs(df[column] - value)
                df_sorted = df.nsmallest(10, 'distance').drop(columns=['distance'])
                return df_sorted

        elif condition['type'] == 'range':
            value1 = condition['value1']
            value2 = condition['value2']
            center = (value1 + value2) / 2
            df['distance'] = abs(df[column] - center)
            df_sorted = df.nsmallest(10, 'distance').drop(columns=['distance'])
            return df_sorted

        return df.head(10)  # Default fallback

    # Check if 'polarity' column exists and has a condition
    if 'polarity' in df_reduced.columns and 'polarity' in search_criteria:
        condition_str = search_criteria['polarity']
        condition = parse_condition(condition_str)

        if condition['type'] == 'not_equal':
            df_reduced = compute_farthest(df_reduced, 'polarity', condition['value'])

        elif condition['type'] == 'single':
            df_reduced = compute_closest(df_reduced, 'polarity', condition)

        elif condition['type'] == 'range':
            df_reduced = compute_closest(df_reduced, 'polarity', condition)

        else:
            # Unrecognized condition; proceed without reducing
            print(f"Unrecognized polarity condition: '{condition_str}'")

    else:
        # Check for 'lat' and 'lng' columns
        lat_condition = search_criteria.get('lat', None) if 'lat' in df_reduced.columns else None
        lng_condition = search_criteria.get('lng', None) if 'lng' in df_reduced.columns else None

        # Initialize distance scores
        distance_scores = pd.Series([0] * len(df_reduced), index=df_reduced.index)

        if lat_condition or lng_condition:
            if lat_condition:
                condition = parse_condition(lat_condition)
                if condition['type'] == 'not_equal':
                    # Farthest from value
                    df_reduced = compute_farthest(df_reduced, 'lat', condition['value'])
                else:
                    # Closest based on condition
                    closest_lat = compute_closest(df_reduced, 'lat', condition)
                    # Merge scores by intersecting
                    distance_scores += (df_reduced.index.isin(closest_lat.index)).astype(int)

            if lng_condition:
                condition = parse_condition(lng_condition)
                if condition['type'] == 'not_equal':
                    # Farthest from value
                    df_reduced = compute_farthest(df_reduced, 'lng', condition['value'])
                else:
                    # Closest based on condition
                    closest_lng = compute_closest(df_reduced, 'lng', condition)
                    # Merge scores by intersecting
                    distance_scores += (df_reduced.index.isin(closest_lng.index)).astype(int)

            # If both 'lat' and 'lng' conditions were applied, prioritize rows that satisfy both
            if lat_condition and lng_condition:
                # Select rows that have both scores
                df_reduced = df_reduced[distance_scores >= 2].head(10)

        else:
            # No 'polarity', 'lat', or 'lng' conditions; select 10 random rows
            df_reduced = df_reduced.sample(n=10, random_state=42)

    # If no numerical conditions matched, and df_reduced is still too large, pick 10 random rows
    if len(df_reduced) > 10:
        df_reduced = df_reduced.head(10)

    return df_reduced


def generate_response(context, question):
    system_instructions = """
    You are an AI assistant that provides helpful, detailed answers to user questions based on the given context.
    When you respond to a question respond intelligently with relevant info. If contributing additional makes the response
    sound unclean and wordy then don't include it. However please provide ample evidence for your answers. Making a claim without evidence is bad.
    Now all restaurant names should be put in ** **. 
    So for example in the response, Osteria die Zaffo should be **Osteria die Zaffo**
    Do not name resturants in this format:
    "Restaurant with place_id 117834"
    If this is all you have say an unnamed restaurant
    """

    user_prompt = f"""
    Context:
    {context}

    Question: {question}

    A:
    """

    try:
        # Get the OpenAI model
        llm = get_open_ai(temperature=0.7, model="gpt-4o")  # or "gpt-3.5-turbo-1106"

        # Create messages
        messages = [
            {"role": "system", "content": system_instructions},
            {"role": "user", "content": user_prompt}
        ]

        # Generate response
        response = llm.invoke(messages)

        # Extract the content from the response
        return response.content.strip()

    except Exception as e:
        print(f"An error occurred: {e}")
        return None


def find_location_in_response(response, df):
    """
    Extracts substrings enclosed within ** **, [] or {} in the response and returns the first substring
    that exactly matches a 'place_name' in the DataFrame. The match is case-insensitive.

    :param response: The text response to search within.
    :param df: The DataFrame containing the 'place_name' column.
    :return: The matched location name with original casing or None if no match is found.
    """
    # Ensure 'place_name' column exists
    if 'place_name' not in df.columns:
        print("Error: 'place_name' column does not exist in the DataFrame.")
        return None

    # Extract unique place names and create a mapping for case-insensitive matching
    place_names = df['place_name'].dropna().unique().tolist()
    place_name_mapping = {name.lower(): name for name in place_names}  # Mapping: lowercase -> original

    # Define regex patterns for ** **, [] and {}
    patterns = [
        r'\*\*(.*?)\*\*',  # ** **
        r'\[(.*?)\]',      # []
        r'\{(.*?)\}'       # {}
    ]

    # Combine all patterns into one regex
    combined_pattern = '|'.join(patterns)

    # Extract all substrings matching the patterns
    enclosed_substrings = re.findall(combined_pattern, response)
    
    # Since re.findall with multiple groups returns tuples, flatten the list
    enclosed_substrings = [item for sublist in enclosed_substrings for item in sublist if item]

    if not enclosed_substrings:
        print("No substrings enclosed by '** **', '[]', or '{}' found in the response.")
        return None

    # Iterate through each enclosed substring
    for substring in enclosed_substrings:
        # Clean the substring: strip whitespace and lowercase for case-insensitive matching
        substring_clean = substring.strip().lower()

        # Check if the cleaned substring exists in the place_names mapping
        if substring_clean in place_name_mapping:
            matched_location = place_name_mapping[substring_clean]
            print(f"Matched Location Found: '{matched_location}'")
            return matched_location

    # If no matches found in any enclosed substrings, return None
    print("No matching location found in the specified sections.")
    return None

def create_map_for_location(df, location_name, max_locations=500):
    """
    Creates a map for the given location and nearby places.
    """
    location = df[df['place_name'] == location_name]
    
    if location.empty:
        return None
    
    location = location.iloc[0]
    
    nearby = df[
        (df['lat'].between(location['lat'] - 0.01, location['lat'] + 0.01)) &
        (df['lng'].between(location['lng'] - 0.01, location['lng'] + 0.01))
    ]
    
    if len(nearby) > max_locations:
        nearby = nearby.sample(n=max_locations, random_state=42)
    
    m = folium.Map(location=[location['lat'], location['lng']], zoom_start=14)
    marker_cluster = MarkerCluster().add_to(m)
    
    for _, place in nearby.iterrows():
        reviews = eval(place['reviews_text']) if isinstance(place['reviews_text'], str) else place['reviews_text']
        sentiment = np.mean([TextBlob(review).sentiment.polarity for review in reviews]) if reviews else 0
        color = f'#{int(255 * (1 - sentiment))//2:02x}{int(255 * (1 + sentiment))//2:02x}00'
        
        folium.CircleMarker(
            location=[place['lat'], place['lng']],
            radius=5,
            popup=f"{place['place_name']}<br>Sentiment: {sentiment:.2f}",
            color=color,
            fill=True,
            fillColor=color
        ).add_to(marker_cluster)
    
    return m

@app.route('/')
def home():
    return 'Hello, World!'

@app.route('/ask', methods=['POST'])
def ask():
    try:
        server = 'openai'
        # model = 'gpt-3.5-turbo-1106'
        model = 'gpt-4o'
        model_endpoint = None
        iterations = 20

        start_time = time.time()
        
        data = request.json
        question = data.get('question')

        if not question:
            return jsonify({"error": "Question is required"}), 400

        location_data_frames = load_collection_data_frames("location_data_frames.pkl")

        column_names = location_data_frames.columns

        graph = create_graph(server=server, model=model, model_endpoint=model_endpoint, question=question, column_names=column_names)
        
        workflow = compile_workflow(graph)
        
        limit = {"recursion_limit": iterations}
        dict_inputs = {"query_question": question}

        verbose = False

        for event in workflow.stream(dict_inputs, limit):
            if verbose:
                print("\nState Dictionary:", event)
            else:
                print("\n")


        with open("criteria.txt", "r") as file:
            search_criteria = json.load(file)
        
        
        result_search = search_database(location_data_frames, search_criteria)

        reduced_df = reduce_dataframe(result_search, search_criteria)

        context = f"Q: {question}\nRelevant Info to answer Q: {reduced_df}\n"

        response = generate_response(context, question)

        location_name = find_location_in_response(response, location_data_frames)
        map_html = None
        if location_name:
            map_obj = create_map_for_location(location_data_frames, location_name)
            if map_obj:
                map_html = map_obj._repr_html_()


        elapsed_time = time.time() - start_time
        print(f"Elapsed time: {elapsed_time:.2f} seconds")

        response_data = {
            "response": response,
            "map_html": map_html,
            "location_name": location_name
        }

        print(response_data)
        return jsonify(response_data)
    except Exception as e:
        error_message = str(e)
        print(f"Error in /ask endpoint: {error_message}")
        return jsonify({"error": error_message}), 400

if __name__ == "__main__":
    # app.run(host='0.0.0.0', port=4000)
    app.run(debug=True, port=4000)

