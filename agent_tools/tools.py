import pandas as pd
import pickle
import json
import re
from typing import Optional, List, Union, Dict
from pydantic import BaseModel, ValidationError, validator, root_validator
from rapidfuzz import process, fuzz

def load_collection_data_frames(filename='collection_data_frames.pkl'):
    """
    Loads the pickled DataFrame from the specified file.

    :param filename: Path to the pickle file.
    :return: Loaded Pandas DataFrame.
    """
    try:
        with open(filename, 'rb') as file:
            data_frames = pickle.load(file)
        print(f"Data successfully loaded from {filename}")
        return data_frames
    except Exception as e:
        print(f"Error loading pickle file: {e}")
        raise e

location_data_frames = load_collection_data_frames("location_data_frames.pkl")

def get_suggestions(value: str, options: List[str], limit: int = 2) -> List[str]:
    """
    Returns a list of suggested strings from options that are similar to the input value.

    :param value: The input string to find suggestions for.
    :param options: The list of possible strings to compare against.
    :param limit: Maximum number of suggestions to return.
    :return: List of suggested strings.
    """
    suggestions = process.extract(value, options, scorer=fuzz.WRatio, limit=limit)
    return [s[0] for s in suggestions if s[1] >= 50]

def is_valid_condition(cond_str: str) -> bool:
    """
    Validates if the condition string matches the allowed formats.

    :param cond_str: Condition string (e.g., "0<x<50.4", "x<25").
    :return: True if valid, False otherwise.
    """
    patterns = [
        r'^x<=?\d+(\.\d+)?$',                 # x<25, x<=25
        r'^x>=?\d+(\.\d+)?$',                 # x>25, x>=25
        r'^x==?\d+(\.\d+)?$',                 # x==25, x=25
        r'^x!=?\d+(\.\d+)?$',                 # x!=25
        r'^\d+(\.\d+)?<x<\d+(\.\d+)?$',       # 0<x<50.4
        r'^\d+(\.\d+)?<=x<=\d+(\.\d+)?$',     # 0<=x<=50.4
        r'^\d+(\.\d+)?<x<=\d+(\.\d+)?$',      # 0<x<=50.4
        r'^\d+(\.\d+)?<=x<\d+(\.\d+)?$'       # 0<=x<50.4
    ]
    for pattern in patterns:
        if re.match(pattern, cond_str):
            return True
    return False

def is_mathematically_sound(cond_str: str) -> bool:
    """
    Checks if the range condition is mathematically sound (lower bound < upper bound).

    :param cond_str: Condition string (e.g., "30<x<50").
    :return: True if sound, False otherwise.
    """
    # Match patterns like "30<x<50", "30<=x<=50", etc.
    match = re.match(r'^(\d+(\.\d+)?)([<>]=?)x([<>]=?)(\d+(\.\d+)?)$', cond_str)
    if match:
        lower = float(match.group(1))
        upper = float(match.group(5))
        return lower < upper
    return True  # Non-range conditions are considered sound

def is_range_condition(cond_str: str) -> bool:
    """
    Determines if the condition string is a range condition.

    :param cond_str: Condition string.
    :return: True if range condition, False otherwise.
    """
    return bool(re.match(r'^\d+(\.\d+)?[<>]=?x[<>]=?\d+(\.\d+)?$', cond_str))



def suggest_condition(condition: str) -> Optional[str]:
    """
    Suggests a corrected condition format based on the input.
    
    :param condition: The invalid condition string.
    :return: Suggested condition string or None if no suggestion is found.
    """
    # Define valid condition patterns
    valid_conditions = [
        "x<25", "x<=25", "x>25", "x>=25", "x==25", "x!=25",
        "0<x<50.4", "0<=x<=50.4", "0<x<=50.4", "0<=x<50.4",
        "x<0.5", "x<=0.5", "x>0.5", "x>=0.5"
    ]
    
    # Use RapidFuzz to find the best match
    suggestions = process.extract(condition, valid_conditions, scorer=fuzz.WRatio, limit=1)
    
    if suggestions and suggestions[0][1] >= 60:  # Adjust threshold as needed
        return suggestions[0][0]
    return None

# Just this class generated by chatgpt
class SearchQuery(BaseModel):
    """
    Represents the entire search query, including search conditions and columns to return.
    """
    columns: Optional[Union[str, List[str]]] = 'all'

    class Config:
        extra = 'allow'  # Allow extra fields representing search conditions

    @root_validator(pre=True)
    def validate_search_query(cls, values):
        """
        Validates the search criteria and columns.
        """
        columns = values.get('columns', 'all')

        # Extract search conditions by excluding 'columns'
        conditions = {k: v for k, v in values.items() if k != 'columns'}

        errors = []

        # Validate search condition keys
        for key in conditions.keys():
            if key not in location_data_frames.columns:
                suggestions = get_suggestions(key, location_data_frames.columns.tolist())
                if suggestions:
                    suggestion_str = " or ".join(f"'{s}'" for s in suggestions)
                    errors.append(f"Invalid key '{key}'. Did you mean {suggestion_str}?")
                else:
                    errors.append(f"Invalid key '{key}'. No similar columns found.")

        # Validate search condition values
        for key, value in conditions.items():
            if key in ['lat', 'lng', 'polarity']:
                # Validate numerical condition string
                if not is_valid_condition(value):
                    suggestion = suggest_condition(value)
                    if suggestion:
                        errors.append(f"Invalid condition '{value}' for numerical column '{key}'. Did you mean '{suggestion}'?")
                    else:
                        errors.append(f"Invalid condition '{value}' for numerical column '{key}'.")
                elif is_range_condition(value) and not is_mathematically_sound(value):
                    errors.append(f"Condition '{value}' for column '{key}' is mathematically unsound.")
            else:
                # Validate string value exists in the column
                unique_values = location_data_frames[key].dropna().astype(str).unique().tolist()
                if value not in unique_values:
                    suggestions = get_suggestions(value, unique_values)
                    if suggestions:
                        suggestion_str = " or ".join(f"'{s}'" for s in suggestions)
                        errors.append(f"Value '{value}' not found in column '{key}'. Did you mean {suggestion_str}?")
                    else:
                        errors.append(f"Value '{value}' not found in column '{key}'.")

        # Validate 'columns'
        if isinstance(columns, list):
            for col in columns:
                if col != 'all' and col not in location_data_frames.columns:
                    suggestions = get_suggestions(col, location_data_frames.columns.tolist())
                    if suggestions:
                        suggestion_str = " or ".join(f"'{s}'" for s in suggestions)
                        errors.append(f"Invalid column '{col}'. Did you mean {suggestion_str}?")
                    else:
                        errors.append(f"Invalid column '{col}'. No similar columns found.")
        elif isinstance(columns, str):
            if columns != 'all' and columns not in location_data_frames.columns:
                suggestions = get_suggestions(columns, location_data_frames.columns.tolist())
                if suggestions:
                    suggestion_str = " or ".join(f"'{s}'" for s in suggestions)
                    errors.append(f"Invalid column '{columns}'. Did you mean {suggestion_str}?")
                else:
                    errors.append(f"Invalid column '{columns}'. No similar columns found.")
        else:
            errors.append(f"'columns' must be 'all' or a list of column names.")

        if errors:
            raise ValueError("; ".join(errors))

        return values


class ValidateSearchQueryTool:
    """
    Validates a search query against the Pydantic schemas and saves it to criteria.txt if valid.
    """
    openai_schema = {
        "name": "ValidateSearchQueryTool",
        "description": "Validates a search query against the Pydantic schemas and saves it to criteria.txt if valid",
        "parameters": {
            "type": "object",
            "properties": {
                "search_criteria": {
                    "type": "string",
                    "description": "Search criteria as a JSON string containing a dictionary with search conditions and a 'columns' key"
                }
            },
            "required": ["search_criteria"]
        }
    }

    def __init__(self, search_criteria: str):
        self.search_criteria = search_criteria

    def run(self):
        try:
            # Parse the JSON string to a Python object
            search_criteria_obj = json.loads(self.search_criteria)

            # Ensure it's a dictionary
            if not isinstance(search_criteria_obj, dict):
                raise ValueError("Invalid search criteria format. Must be a dictionary.")

            # Validate using Pydantic
            try:
                search_query = SearchQuery(**search_criteria_obj)
            except ValidationError as e:
                error_messages = []
                for error in e.errors():
                    # For root errors, 'loc' might be empty
                    loc = ".".join(map(str, error['loc'])) if error['loc'] else "root"
                    error_msg = f"{loc}: {error['msg']}"
                    error_messages.append(error_msg)
                return {"error": "\n".join(error_messages)}

            # Prepare the formatted query for saving (flat dictionary)
            formatted_query = search_criteria_obj

            # Save the formatted query to criteria.txt
            with open("criteria.txt", "w") as file:
                json.dump(formatted_query, file, indent=2)

            return {"output": "Search criteria validated successfully and saved to criteria.txt"}

        except json.JSONDecodeError as e:
            return {"error": f"Invalid JSON: {str(e)}"}
        except ValidationError as e:
            error_messages = []
            for error in e.errors():
                # For root errors, 'loc' might be empty
                loc = ".".join(map(str, error['loc'])) if error['loc'] else "root"
                error_msg = f"{loc}: {error['msg']}"
                error_messages.append(error_msg)
            return {"error": f"Invalid search query: {len(e.errors())} validation error(s)\n" + "\n".join(error_messages)}
        except Exception as e:
            error_message = str(e)
            print(f"Error in ValidateSearchQueryTool: {error_message}")
            return {"error": error_message}


def search_database(df: pd.DataFrame, search_query: Dict[str, Union[str, List[str]]]) -> pd.DataFrame:
    """
    Searches the DataFrame based on the provided search criteria.

    :param df: Pandas DataFrame to search.
    :param search_query: Dictionary containing search conditions and 'columns' key.
    :return: Filtered Pandas DataFrame based on the search criteria.
    """
    # Extract 'columns' and search conditions
    columns = search_query.get('columns', 'all')
    search_conditions = {k: v for k, v in search_query.items() if k != 'columns'}

    mask = pd.Series([True] * len(df))

    for key, value in search_conditions.items():
        if key in ['lat', 'lng', 'polarity']:
            # Handle numerical conditions
            # Parse the condition string
            # Patterns: 'x<25', 'x<=25', 'x>25', 'x>=25', 'x==25', 'x!=25', '50<x<53', etc.
            # Using regular expressions to parse

            # Single condition
            single_cond_match = re.match(r'^x([<>]=?)(\d+(\.\d+)?)$', value)
            if single_cond_match:
                operator, num_str, _ = single_cond_match.groups()
                num = float(num_str)
                if operator == '<':
                    mask &= df[key] < num
                elif operator == '<=':
                    mask &= df[key] <= num
                elif operator == '>':
                    mask &= df[key] > num
                elif operator == '>=':
                    mask &= df[key] >= num
                elif operator == '==':
                    mask &= df[key] == num
                elif operator == '!=':
                    mask &= df[key] != num
                continue

            # Range condition
            range_cond_match = re.match(r'^(\d+(\.\d+)?)([<>]=?)x([<>]=?)(\d+(\.\d+)?$)', value)
            if range_cond_match:
                lower_str, _, op1, op2, upper_str, _ = range_cond_match.groups()
                lower = float(lower_str)
                upper = float(upper_str)
                # Determine if inclusive or exclusive based on operators
                if op1 in ['<', '<=']:
                    lower_condition = df[key] > lower if op1 == '<' else df[key] >= lower
                else:
                    lower_condition = df[key] > lower if op1 == '>' else df[key] >= lower

                if op2 in ['<', '<=']:
                    upper_condition = df[key] < upper if op2 == '<' else df[key] <= upper
                else:
                    upper_condition = df[key] < upper if op2 == '<' else df[key] <= upper

                mask &= lower_condition & upper_condition
                continue

            # If no match, skip or handle as error
            print(f"Unrecognized condition format for key '{key}': '{value}'. Skipping this condition.")
            continue

        else:
            # Handle string exact matches
            mask &= df[key] == value

    # Apply the mask to filter the DataFrame
    filtered_df = df[mask]

    # Handle 'columns' to return
    if isinstance(columns, list):
        # Validate and select specified columns
        valid_columns = [col for col in columns if col in df.columns]
        if not valid_columns:
            print("No valid columns specified. Returning all columns.")
            return filtered_df
        filtered_df = filtered_df[valid_columns]
    elif isinstance(columns, str):
        if columns == 'all':
            pass  # all columns already selected
        elif columns in df.columns:
            filtered_df = filtered_df[[columns]]
        else:
            print(f"Invalid columns specification '{columns}'. Returning all columns.")
    else:
        print(f"'columns' must be 'all' or a list of column names. Returning all columns.")

    return filtered_df
