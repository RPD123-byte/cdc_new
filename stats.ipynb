{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully loaded from location_data_frames.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "def load_collection_data_frames(filename='collection_data_frames.pkl'):\n",
    "    \"\"\"\n",
    "    Loads the pickled DataFrame from the specified file.\n",
    "\n",
    "    :param filename: Path to the pickle file.\n",
    "    :return: Loaded Pandas DataFrame.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filename, 'rb') as file:\n",
    "            data_frames = pickle.load(file)\n",
    "        print(f\"Data successfully loaded from {filename}\")\n",
    "        return data_frames\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading pickle file: {e}\")\n",
    "        raise e\n",
    "    \n",
    "location_data_frames = load_collection_data_frames(\"location_data_frames.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 492880 entries, 0 to 492879\n",
      "Data columns (total 13 columns):\n",
      " #   Column                      Non-Null Count   Dtype  \n",
      "---  ------                      --------------   -----  \n",
      " 0   location                    492880 non-null  object \n",
      " 1   category                    492880 non-null  object \n",
      " 2   place_id                    492880 non-null  int64  \n",
      " 3   place_name                  492880 non-null  object \n",
      " 4   reviews_text                492880 non-null  object \n",
      " 5   address                     411474 non-null  object \n",
      " 6   international_phone_number  260175 non-null  object \n",
      " 7   lat                         474222 non-null  float64\n",
      " 8   lng                         474222 non-null  float64\n",
      " 9   polarity                    200918 non-null  float64\n",
      " 10  website                     214159 non-null  object \n",
      " 11  reviews_count               492880 non-null  int64  \n",
      " 12  has_website                 492880 non-null  bool   \n",
      "dtypes: bool(1), float64(3), int64(2), object(7)\n",
      "memory usage: 45.6+ MB\n",
      "None\n",
      "\n",
      "Sample of the data:\n",
      "    location       category  place_id                              place_name  \\\n",
      "0  Amsterdam  accommodation    223896                       Hotel Cornerhouse   \n",
      "1  Amsterdam  accommodation    223771                          American Hotel   \n",
      "2  Amsterdam  accommodation    223793                                NL Hotel   \n",
      "3  Amsterdam  accommodation    223778                         Hotel Amsterdam   \n",
      "4  Amsterdam  accommodation    223829  Amsterdam Bed and Breakfast CityCenter   \n",
      "\n",
      "                                        reviews_text  \\\n",
      "0                                                 []   \n",
      "1  [We went here for the sunday jazz brunch. The ...   \n",
      "2                                                 []   \n",
      "3                                                 []   \n",
      "4  [If anybody is searching for a great place to ...   \n",
      "\n",
      "                                             address  \\\n",
      "0    Burgwallen-Nieuwe Zijde, Amsterdam, Netherlands   \n",
      "1              Leidsekade 97, Amsterdam, Netherlands   \n",
      "2  Frans van Mierisstraat 34, Amsterdam, Netherlands   \n",
      "3               Damrak 93-94, Amsterdam, Netherlands   \n",
      "4       Sint Jacobsstraat 21, Amsterdam, Netherlands   \n",
      "\n",
      "  international_phone_number        lat       lng  polarity  \\\n",
      "0            +31 20 624 1326  52.374310  4.891450       NaN   \n",
      "1            +31 20 556 3000  52.363826  4.881369       8.0   \n",
      "2            +31 20 679 8995  52.354664  4.882476       NaN   \n",
      "3            +31 20 555 0666  52.373690  4.893623       NaN   \n",
      "4             +31 6 45160078  52.376568  4.894706      10.0   \n",
      "\n",
      "                                             website  reviews_count  \\\n",
      "0                    http://www.hotelcornerhouse.nl/              0   \n",
      "1  http://www.hampshire-hotels.com/amsterdam-amer...              5   \n",
      "2  http://museumplein.nl-hotel.com/NL-Hotel-Museu...              0   \n",
      "3                      http://www.hotelamsterdam.nl/              0   \n",
      "4                http://www.citycenter-amsterdam.com              5   \n",
      "\n",
      "   has_website  \n",
      "0         True  \n",
      "1         True  \n",
      "2         True  \n",
      "3         True  \n",
      "4         True  \n",
      "\n",
      "Basic statistics:\n",
      "            place_id            lat            lng       polarity  \\\n",
      "count  492880.000000  474222.000000  474222.000000  200918.000000   \n",
      "mean   287086.523486      47.141129       7.998998       3.333240   \n",
      "std    155216.312446       6.151245      11.217213       3.742147   \n",
      "min         1.000000      24.627216      -0.501959       0.000000   \n",
      "25%    172272.750000      43.466968      -0.027706       0.000000   \n",
      "50%    296507.500000      48.882009       4.883632       1.000000   \n",
      "75%    419727.250000      51.547974      12.329621       6.000000   \n",
      "max    542947.000000      52.666763      56.140069      10.000000   \n",
      "\n",
      "       reviews_count  \n",
      "count  492880.000000  \n",
      "mean        1.225452  \n",
      "std         6.847998  \n",
      "min         0.000000  \n",
      "25%         0.000000  \n",
      "50%         0.000000  \n",
      "75%         0.000000  \n",
      "max       863.000000  \n",
      "\n",
      "Correlation matrix:\n",
      "                    lat       lng  polarity  reviews_count\n",
      "lat            1.000000 -0.801277  0.061584      -0.000934\n",
      "lng           -0.801277  1.000000 -0.165800       0.000317\n",
      "polarity       0.061584 -0.165800  1.000000       0.183601\n",
      "reviews_count -0.000934  0.000317  0.183601       1.000000\n",
      "\n",
      "T-test results (polarity scores for accommodations with/without websites):\n",
      "t-statistic: 203.90571685092758, p-value: 0.0\n",
      "\n",
      "Chi-square test results (association between having a website and having reviews):\n",
      "chi2-statistic: 3118.488441094376, p-value: 0.0\n",
      "\n",
      "Analysis complete. Visualizations saved as PNG files.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import ast\n",
    "\n",
    "# Load the data\n",
    "df = location_data_frames\n",
    "\n",
    "# Preprocessing\n",
    "def safe_len(x):\n",
    "    try:\n",
    "        if isinstance(x, str):\n",
    "            return len(ast.literal_eval(x))\n",
    "        elif isinstance(x, list):\n",
    "            return len(x)\n",
    "        else:\n",
    "            return 0\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "df['reviews_count'] = df['reviews_text'].apply(safe_len)\n",
    "df['has_website'] = df['website'].notna()\n",
    "df['polarity'] = pd.to_numeric(df['polarity'], errors='coerce')\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(df.info())\n",
    "print(\"\\nSample of the data:\")\n",
    "print(df.head())\n",
    "\n",
    "# Basic statistics\n",
    "print(\"\\nBasic statistics:\")\n",
    "print(df.describe())\n",
    "\n",
    "# Correlation matrix\n",
    "numeric_columns = ['lat', 'lng', 'polarity', 'reviews_count']\n",
    "correlation_matrix = df[numeric_columns].corr()\n",
    "print(\"\\nCorrelation matrix:\")\n",
    "print(correlation_matrix)\n",
    "\n",
    "# Statistical tests\n",
    "\n",
    "# 1. T-test: Compare polarity scores of accommodations with and without websites\n",
    "with_website = df[df['has_website']]['polarity'].dropna()\n",
    "without_website = df[~df['has_website']]['polarity'].dropna()\n",
    "if len(with_website) > 0 and len(without_website) > 0:\n",
    "    t_stat, p_value = stats.ttest_ind(with_website, without_website)\n",
    "    print(f\"\\nT-test results (polarity scores for accommodations with/without websites):\")\n",
    "    print(f\"t-statistic: {t_stat}, p-value: {p_value}\")\n",
    "else:\n",
    "    print(\"\\nNot enough data to perform T-test.\")\n",
    "\n",
    "# 2. Chi-square test: Association between having a website and having reviews\n",
    "contingency_table = pd.crosstab(df['has_website'], df['reviews_count'] > 0)\n",
    "if contingency_table.size > 0:\n",
    "    chi2, p_value, dof, expected = stats.chi2_contingency(contingency_table)\n",
    "    print(f\"\\nChi-square test results (association between having a website and having reviews):\")\n",
    "    print(f\"chi2-statistic: {chi2}, p-value: {p_value}\")\n",
    "else:\n",
    "    print(\"\\nNot enough data to perform Chi-square test.\")\n",
    "\n",
    "# Data Visualizations\n",
    "\n",
    "# 1. Scatter plot of latitude vs longitude, colored by polarity\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(df['lng'], df['lat'], c=df['polarity'], cmap='viridis', alpha=0.6)\n",
    "plt.colorbar(scatter, label='Polarity')\n",
    "plt.title('Accommodation Locations Colored by Polarity')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.savefig('location_polarity_scatter.png')\n",
    "plt.close()\n",
    "\n",
    "# 2. Box plot of polarity scores for accommodations with and without websites\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(x='has_website', y='polarity', data=df)\n",
    "plt.title('Polarity Scores for Accommodations With and Without Websites')\n",
    "plt.savefig('website_polarity_boxplot.png')\n",
    "plt.close()\n",
    "\n",
    "# 3. Histogram of review counts\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(df['reviews_count'], bins=20, edgecolor='black')\n",
    "plt.title('Distribution of Review Counts')\n",
    "plt.xlabel('Number of Reviews')\n",
    "plt.ylabel('Frequency')\n",
    "plt.savefig('review_count_histogram.png')\n",
    "plt.close()\n",
    "\n",
    "print(\"\\nAnalysis complete. Visualizations saved as PNG files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map created for Osteria dei Pazzi with 500 nearby locations (max 500). Saved as 'Osteria dei Pazzi_map.html'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from textblob import TextBlob\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "\n",
    "def map_location(df, location_name, max_locations=500):\n",
    "    # Find the specified location\n",
    "    location = df[df['place_name'] == location_name]\n",
    "    \n",
    "    if location.empty:\n",
    "        return f\"Location '{location_name}' not found.\"\n",
    "    \n",
    "    location = location.iloc[0]\n",
    "    \n",
    "    # Find nearby locations (within ~1km)\n",
    "    nearby = df[\n",
    "        (df['lat'].between(location['lat'] - 0.01, location['lat'] + 0.01)) &\n",
    "        (df['lng'].between(location['lng'] - 0.01, location['lng'] + 0.01))\n",
    "    ]\n",
    "    \n",
    "    # Limit to max_locations\n",
    "    if len(nearby) > max_locations:\n",
    "        nearby = nearby.sample(n=max_locations, random_state=42)\n",
    "    \n",
    "    # Create map\n",
    "    m = folium.Map(location=[location['lat'], location['lng']], zoom_start=14)\n",
    "    \n",
    "    # Use MarkerCluster for better performance with many markers\n",
    "    marker_cluster = MarkerCluster().add_to(m)\n",
    "    \n",
    "    # Add markers\n",
    "    for _, place in nearby.iterrows():\n",
    "        # Simple sentiment analysis\n",
    "        reviews = eval(place['reviews_text']) if isinstance(place['reviews_text'], str) else place['reviews_text']\n",
    "        sentiment = np.mean([TextBlob(review).sentiment.polarity for review in reviews]) if reviews else 0\n",
    "        \n",
    "        # Color based on sentiment (-1 to 1 scale)\n",
    "        color = f'#{int(255 * (1 - sentiment))//2:02x}{int(255 * (1 + sentiment))//2:02x}00'\n",
    "        \n",
    "        folium.CircleMarker(\n",
    "            location=[place['lat'], place['lng']],\n",
    "            radius=5,\n",
    "            popup=f\"{place['place_name']}<br>Sentiment: {sentiment:.2f}\",\n",
    "            color=color,\n",
    "            fill=True,\n",
    "            fillColor=color\n",
    "        ).add_to(marker_cluster)\n",
    "    \n",
    "    # Save map\n",
    "    m.save(f\"{location_name}_map.html\")\n",
    "    \n",
    "    return f\"Map created for {location_name} with {len(nearby)} nearby locations (max 500). Saved as '{location_name}_map.html'.\"\n",
    "\n",
    "# Example usage\n",
    "location_name = \"Osteria dei Pazzi\"\n",
    "result = map_location(location_data_frames, location_name)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nKMeans does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/rithvikprakki/CDC_2024/stats.ipynb Cell 3\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rithvikprakki/CDC_2024/stats.ipynb#W2sZmlsZQ%3D%3D?line=94'>95</a>\u001b[0m             \u001b[39mreturn\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mI\u001b[39m\u001b[39m'\u001b[39m\u001b[39mm sorry, I don\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt understand that question. Can you please rephrase it?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rithvikprakki/CDC_2024/stats.ipynb#W2sZmlsZQ%3D%3D?line=96'>97</a>\u001b[0m \u001b[39m# Usage\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rithvikprakki/CDC_2024/stats.ipynb#W2sZmlsZQ%3D%3D?line=97'>98</a>\u001b[0m chatbot \u001b[39m=\u001b[39m EnhancedTravelChatbot(location_data_frames)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rithvikprakki/CDC_2024/stats.ipynb#W2sZmlsZQ%3D%3D?line=98'>99</a>\u001b[0m response \u001b[39m=\u001b[39m chatbot\u001b[39m.\u001b[39manswer_question(\u001b[39m\"\u001b[39m\u001b[39mWhat are the best places in Rome?\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/rithvikprakki/CDC_2024/stats.ipynb#W2sZmlsZQ%3D%3D?line=99'>100</a>\u001b[0m \u001b[39mprint\u001b[39m(response)\n",
      "\u001b[1;32m/Users/rithvikprakki/CDC_2024/stats.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rithvikprakki/CDC_2024/stats.ipynb#W2sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdf \u001b[39m=\u001b[39m data\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rithvikprakki/CDC_2024/stats.ipynb#W2sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess_data()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rithvikprakki/CDC_2024/stats.ipynb#W2sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49manalyze_data()\n",
      "\u001b[1;32m/Users/rithvikprakki/CDC_2024/stats.ipynb Cell 3\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rithvikprakki/CDC_2024/stats.ipynb#W2sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m coords \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdf[[\u001b[39m'\u001b[39m\u001b[39mlat\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mlng\u001b[39m\u001b[39m'\u001b[39m]]\u001b[39m.\u001b[39mvalues\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rithvikprakki/CDC_2024/stats.ipynb#W2sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m kmeans \u001b[39m=\u001b[39m KMeans(n_clusters\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rithvikprakki/CDC_2024/stats.ipynb#W2sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdf[\u001b[39m'\u001b[39m\u001b[39mcluster\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m kmeans\u001b[39m.\u001b[39;49mfit_predict(coords)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rithvikprakki/CDC_2024/stats.ipynb#W2sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39m# Calculate price ranges (assuming you have a 'price' column)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rithvikprakki/CDC_2024/stats.ipynb#W2sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mprice\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdf\u001b[39m.\u001b[39mcolumns:\n",
      "File \u001b[0;32m~/CDC_2024/cdc/lib/python3.12/site-packages/sklearn/cluster/_kmeans.py:1070\u001b[0m, in \u001b[0;36m_BaseKMeans.fit_predict\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit_predict\u001b[39m(\u001b[39mself\u001b[39m, X, y\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   1048\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute cluster centers and predict cluster index for each sample.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \n\u001b[1;32m   1050\u001b[0m \u001b[39m    Convenience method; equivalent to calling fit(X) followed by\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1068\u001b[0m \u001b[39m        Index of the cluster each sample belongs to.\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1070\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(X, sample_weight\u001b[39m=\u001b[39;49msample_weight)\u001b[39m.\u001b[39mlabels_\n",
      "File \u001b[0;32m~/CDC_2024/cdc/lib/python3.12/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/CDC_2024/cdc/lib/python3.12/site-packages/sklearn/cluster/_kmeans.py:1464\u001b[0m, in \u001b[0;36mKMeans.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1436\u001b[0m \u001b[39m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m   1437\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   1438\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute k-means clustering.\u001b[39;00m\n\u001b[1;32m   1439\u001b[0m \n\u001b[1;32m   1440\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1462\u001b[0m \u001b[39m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m   1463\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1464\u001b[0m     X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[1;32m   1465\u001b[0m         X,\n\u001b[1;32m   1466\u001b[0m         accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1467\u001b[0m         dtype\u001b[39m=\u001b[39;49m[np\u001b[39m.\u001b[39;49mfloat64, np\u001b[39m.\u001b[39;49mfloat32],\n\u001b[1;32m   1468\u001b[0m         order\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mC\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1469\u001b[0m         copy\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcopy_x,\n\u001b[1;32m   1470\u001b[0m         accept_large_sparse\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_params_vs_input(X)\n\u001b[1;32m   1475\u001b[0m     random_state \u001b[39m=\u001b[39m check_random_state(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrandom_state)\n",
      "File \u001b[0;32m~/CDC_2024/cdc/lib/python3.12/site-packages/sklearn/base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    631\u001b[0m         out \u001b[39m=\u001b[39m X, y\n\u001b[1;32m    632\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 633\u001b[0m     out \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[1;32m    634\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n\u001b[1;32m    635\u001b[0m     out \u001b[39m=\u001b[39m _check_y(y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n",
      "File \u001b[0;32m~/CDC_2024/cdc/lib/python3.12/site-packages/sklearn/utils/validation.py:1064\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1058\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1059\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound array with dim \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m expected <= 2.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1060\u001b[0m         \u001b[39m%\u001b[39m (array\u001b[39m.\u001b[39mndim, estimator_name)\n\u001b[1;32m   1061\u001b[0m     )\n\u001b[1;32m   1063\u001b[0m \u001b[39mif\u001b[39;00m force_all_finite:\n\u001b[0;32m-> 1064\u001b[0m     _assert_all_finite(\n\u001b[1;32m   1065\u001b[0m         array,\n\u001b[1;32m   1066\u001b[0m         input_name\u001b[39m=\u001b[39;49minput_name,\n\u001b[1;32m   1067\u001b[0m         estimator_name\u001b[39m=\u001b[39;49mestimator_name,\n\u001b[1;32m   1068\u001b[0m         allow_nan\u001b[39m=\u001b[39;49mforce_all_finite \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1069\u001b[0m     )\n\u001b[1;32m   1071\u001b[0m \u001b[39mif\u001b[39;00m copy:\n\u001b[1;32m   1072\u001b[0m     \u001b[39mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[1;32m   1073\u001b[0m         \u001b[39m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[0;32m~/CDC_2024/cdc/lib/python3.12/site-packages/sklearn/utils/validation.py:123\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[39mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    121\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m _assert_all_finite_element_wise(\n\u001b[1;32m    124\u001b[0m     X,\n\u001b[1;32m    125\u001b[0m     xp\u001b[39m=\u001b[39;49mxp,\n\u001b[1;32m    126\u001b[0m     allow_nan\u001b[39m=\u001b[39;49mallow_nan,\n\u001b[1;32m    127\u001b[0m     msg_dtype\u001b[39m=\u001b[39;49mmsg_dtype,\n\u001b[1;32m    128\u001b[0m     estimator_name\u001b[39m=\u001b[39;49mestimator_name,\n\u001b[1;32m    129\u001b[0m     input_name\u001b[39m=\u001b[39;49minput_name,\n\u001b[1;32m    130\u001b[0m )\n",
      "File \u001b[0;32m~/CDC_2024/cdc/lib/python3.12/site-packages/sklearn/utils/validation.py:172\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[39mif\u001b[39;00m estimator_name \u001b[39mand\u001b[39;00m input_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    156\u001b[0m     \u001b[39m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     \u001b[39m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     msg_err \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[1;32m    159\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m does not accept missing values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m#estimators-that-handle-nan-values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    171\u001b[0m     )\n\u001b[0;32m--> 172\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input X contains NaN.\nKMeans does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "\n",
    "class EnhancedTravelChatbot:\n",
    "    def __init__(self, data):\n",
    "        self.df = data\n",
    "        self.preprocess_data()\n",
    "        self.analyze_data()\n",
    "    \n",
    "    def preprocess_data(self):\n",
    "        # Convert reviews to list if it's a string\n",
    "        self.df['reviews_list'] = self.df['reviews_text'].apply(lambda x: eval(x) if isinstance(x, str) else x)\n",
    "        \n",
    "        # Calculate average polarity of reviews\n",
    "        self.df['avg_polarity'] = self.df['reviews_list'].apply(self.get_average_sentiment)\n",
    "        \n",
    "        # Count number of reviews\n",
    "        self.df['review_count'] = self.df['reviews_list'].apply(len)\n",
    "\n",
    "    def analyze_data(self):\n",
    "        # Perform K-means clustering on locations\n",
    "        coords = self.df[['lat', 'lng']].values\n",
    "        kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "        self.df['cluster'] = kmeans.fit_predict(coords)\n",
    "\n",
    "        # Calculate price ranges (assuming you have a 'price' column)\n",
    "        if 'price' in self.df.columns:\n",
    "            self.df['price_category'] = pd.qcut(self.df['price'], q=3, labels=['Budget', 'Mid-range', 'Luxury'])\n",
    "\n",
    "    def get_average_sentiment(self, reviews):\n",
    "        if not reviews:\n",
    "            return 0\n",
    "        sentiments = [TextBlob(review).sentiment.polarity for review in reviews]\n",
    "        return np.mean(sentiments)\n",
    "\n",
    "    def find_best_places(self, city, top_n=5):\n",
    "        city_df = self.df[self.df['location'] == city]\n",
    "        return city_df.nlargest(top_n, 'avg_polarity')\n",
    "\n",
    "    def find_places_in_range(self, min_lat, max_lat, min_lng, max_lng):\n",
    "        return self.df[\n",
    "            (self.df['lat'] >= min_lat) & (self.df['lat'] <= max_lat) &\n",
    "            (self.df['lng'] >= min_lng) & (self.df['lng'] <= max_lng)\n",
    "        ]\n",
    "\n",
    "    def get_popular_amenities(self, city):\n",
    "        city_df = self.df[self.df['location'] == city]\n",
    "        all_amenities = [amenity for amenities in city_df['amenities'].dropna() for amenity in amenities.split(',')]\n",
    "        return pd.Series(all_amenities).value_counts().head(10)\n",
    "\n",
    "    def recommend_similar_places(self, place_id):\n",
    "        place = self.df[self.df['place_id'] == place_id].iloc[0]\n",
    "        cluster = place['cluster']\n",
    "        similar_places = self.df[\n",
    "            (self.df['cluster'] == cluster) & \n",
    "            (self.df['place_id'] != place_id)\n",
    "        ].sort_values('avg_polarity', ascending=False)\n",
    "        return similar_places.head(5)\n",
    "\n",
    "    def answer_question(self, question):\n",
    "        if \"best places in\" in question.lower():\n",
    "            city = re.search(r\"best places in (.+)\", question, re.IGNORECASE).group(1)\n",
    "            best_places = self.find_best_places(city)\n",
    "            return f\"The top 5 places in {city} based on reviews are:\\n\" + \\\n",
    "                   \"\\n\".join([f\"{i+1}. {row['place_name']} (Avg. Polarity: {row['avg_polarity']:.2f})\" \n",
    "                              for i, (_, row) in enumerate(best_places.iterrows())])\n",
    "\n",
    "        elif \"between\" in question.lower() and \"latitude\" in question.lower() and \"longitude\" in question.lower():\n",
    "            coords = re.findall(r\"(-?\\d+\\.?\\d*)\", question)\n",
    "            if len(coords) == 4:\n",
    "                places = self.find_places_in_range(*map(float, coords))\n",
    "                return f\"Found {len(places)} places in the specified range. Here are the top 5:\\n\" + \\\n",
    "                       \"\\n\".join([f\"{i+1}. {row['place_name']} ({row['lat']}, {row['lng']})\" \n",
    "                                  for i, (_, row) in enumerate(places.head().iterrows())])\n",
    "\n",
    "        elif \"popular amenities\" in question.lower():\n",
    "            city = re.search(r\"popular amenities in (.+)\", question, re.IGNORECASE).group(1)\n",
    "            amenities = self.get_popular_amenities(city)\n",
    "            return f\"The most popular amenities in {city} are:\\n\" + \\\n",
    "                   \"\\n\".join([f\"{i+1}. {amenity} ({count} places)\" for i, (amenity, count) in enumerate(amenities.items())])\n",
    "\n",
    "        elif \"similar to\" in question.lower():\n",
    "            place_name = re.search(r\"similar to (.+)\", question, re.IGNORECASE).group(1)\n",
    "            place_id = self.df[self.df['place_name'] == place_name]['place_id'].iloc[0]\n",
    "            similar_places = self.recommend_similar_places(place_id)\n",
    "            return f\"Places similar to {place_name} are:\\n\" + \\\n",
    "                   \"\\n\".join([f\"{i+1}. {row['place_name']} (Avg. Polarity: {row['avg_polarity']:.2f})\" \n",
    "                              for i, (_, row) in enumerate(similar_places.iterrows())])\n",
    "\n",
    "        else:\n",
    "            return \"I'm sorry, I don't understand that question. Can you please rephrase it?\"\n",
    "\n",
    "# Usage\n",
    "chatbot = EnhancedTravelChatbot(location_data_frames)\n",
    "response = chatbot.answer_question(\"What are the best places in Rome?\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cdc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
