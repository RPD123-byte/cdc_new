{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV expanded successfully: amsterdam_expanded.csv\n",
      "CSV expanded successfully: barcelona_expanded.csv\n",
      "CSV expanded successfully: rome_expanded.csv\n",
      "CSV expanded successfully: dubai_expanded.csv\n",
      "CSV expanded successfully: paris_expanded.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import os\n",
    "\n",
    "# Function to safely evaluate the string dictionary\n",
    "def safe_eval_dict(details_text):\n",
    "    try:\n",
    "        # Convert string representation of dictionary to actual dictionary\n",
    "        return ast.literal_eval(details_text)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return {}\n",
    "\n",
    "# Function to process a single CSV\n",
    "def process_csv(file_path):\n",
    "    # Load the CSV\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Apply the safe_eval_dict function to the 'details_text' column\n",
    "    df['details_dict'] = df['details_text'].apply(safe_eval_dict)\n",
    "\n",
    "    # Normalize the 'details_dict' column to individual columns, filling missing values with NaN\n",
    "    details_df = pd.json_normalize(df['details_dict'])\n",
    "\n",
    "    # Concatenate the original dataframe with the new columns\n",
    "    df_expanded = pd.concat([df.drop(columns=['details_text', 'details_dict']), details_df], axis=1)\n",
    "\n",
    "    # Save the resulting DataFrame to a new CSV file (appending '_expanded' to the original file name)\n",
    "    new_file_path = os.path.splitext(file_path)[0] + '_expanded.csv'\n",
    "    df_expanded.to_csv(new_file_path, index=False)\n",
    "\n",
    "    print(f\"CSV expanded successfully: {new_file_path}\")\n",
    "\n",
    "# List of CSV file paths to process\n",
    "csv_files = [\n",
    "    'amsterdam.csv',\n",
    "    'barcelona.csv',\n",
    "    'rome.csv',\n",
    "    'dubai.csv',\n",
    "    'paris.csv'\n",
    "]\n",
    "\n",
    "\n",
    "# Loop over the list of CSVs and process each one\n",
    "for csv_file in csv_files:\n",
    "    process_csv(csv_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read amsterdam_expanded.csv with 20260 records.\n",
      "Filtered CSV saved: amsterdam_expanded_filtered.csv\n",
      "Successfully read barcelona_expanded.csv with 81950 records.\n",
      "Filtered CSV saved: barcelona_expanded_filtered.csv\n",
      "Successfully read rome_expanded.csv with 156602 records.\n",
      "Filtered CSV saved: rome_expanded_filtered.csv\n",
      "Successfully read dubai_expanded.csv with 177539 records.\n",
      "Filtered CSV saved: dubai_expanded_filtered.csv\n",
      "Successfully read paris_expanded.csv with 56529 records.\n",
      "Filtered CSV saved: paris_expanded_filtered.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def process_and_filter_csv(file_path, columns_to_keep):\n",
    "    \"\"\"\n",
    "    Processes a single CSV, keeping only the specified columns and dropping the rest.\n",
    "\n",
    "    :param file_path: The path to the CSV file to process.\n",
    "    :param columns_to_keep: List of column names to keep in the DataFrame.\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(file_path):\n",
    "        print(f\"Warning: {file_path} does not exist and will be skipped.\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Load the CSV\n",
    "        df = pd.read_csv(file_path, low_memory=False)\n",
    "        print(f\"Successfully read {file_path} with {len(df)} records.\")\n",
    "        \n",
    "        # Keep only the specified columns\n",
    "        df_filtered = df[columns_to_keep]\n",
    "\n",
    "        # Save the filtered CSV with '_filtered' appended to the original file name\n",
    "        filtered_file_path = os.path.splitext(file_path)[0] + '_filtered.csv'\n",
    "        df_filtered.to_csv(filtered_file_path, index=False)\n",
    "        \n",
    "        print(f\"Filtered CSV saved: {filtered_file_path}\")\n",
    "        \n",
    "        return df_filtered\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # List of CSV filenames to process\n",
    "    csv_files = [\n",
    "        'amsterdam_expanded.csv',\n",
    "        'barcelona_expanded.csv',\n",
    "        'rome_expanded.csv',\n",
    "        'dubai_expanded.csv',\n",
    "        'paris_expanded.csv'\n",
    "    ]\n",
    "\n",
    "    # List of columns to keep\n",
    "    columns_to_keep = [\n",
    "        'location',\n",
    "        'category',\n",
    "        'place_id',\n",
    "        'place_name',\n",
    "        'reviews_text',\n",
    "        'address',\n",
    "        'international_phone_number',\n",
    "        'lat',\n",
    "        'lng',\n",
    "        'polarity',\n",
    "        'website'\n",
    "    ]\n",
    "\n",
    "    # Process each CSV file and filter columns\n",
    "    for csv_file in csv_files:\n",
    "        process_and_filter_csv(csv_file, columns_to_keep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read Translated/amsterdam_translated.csv with 20260 records.\n",
      "Successfully read Translated/barcelona_translated.csv with 81950 records.\n",
      "Successfully read Translated/dubai_translated.csv with 177539 records.\n",
      "Successfully read Translated/paris_translated.csv with 56529 records.\n",
      "Successfully read Translated/rome_translated.csv with 156602 records.\n",
      "Combined DataFrame has 492880 records and 11 columns.\n",
      "Parsed column: reviews_text\n",
      "Removed 375330 rows with empty 'reviews_text'. Remaining records: 117550\n",
      "Data successfully saved to location_data_frames.pkl\n",
      "\n",
      "First 5 records of the combined DataFrame:\n",
      "    location       category  place_id                              place_name  \\\n",
      "1  Amsterdam  accommodation    223771                          American Hotel   \n",
      "4  Amsterdam  accommodation    223829  Amsterdam Bed and Breakfast CityCenter   \n",
      "5  Amsterdam  accommodation    223818                              BLUE TOWER   \n",
      "6  Amsterdam  accommodation    223776              Hotel Espresso City Center   \n",
      "7  Amsterdam  accommodation    223779  Hampshire Boutique Hotel - Sebastian's   \n",
      "\n",
      "                                        reviews_text  \\\n",
      "1  [We went here for the sunday jazz brunch. The ...   \n",
      "4  [If anybody is searching for a great place to ...   \n",
      "5  [Value for money, We stayed at this hotel on 2...   \n",
      "6  [Rooms are nice and clean..hotel staff are ext...   \n",
      "7  [My girlfriend and I had a lovely 4-night stay...   \n",
      "\n",
      "                                        address international_phone_number  \\\n",
      "1         Leidsekade 97, Amsterdam, Netherlands            +31 20 556 3000   \n",
      "4  Sint Jacobsstraat 21, Amsterdam, Netherlands             +31 6 45160078   \n",
      "5   Leeuwendalersweg 21, Amsterdam, Netherlands            +31 20 580 0360   \n",
      "6           Overtoom 57, Amsterdam, Netherlands            +31 20 412 0880   \n",
      "7      Keizersgracht 15, Amsterdam, Netherlands            +31 20 423 2342   \n",
      "\n",
      "         lat       lng  polarity  \\\n",
      "1  52.363826  4.881369       8.0   \n",
      "4  52.376568  4.894706      10.0   \n",
      "5  52.379318  4.845886       3.0   \n",
      "6  52.362864  4.876344       0.0   \n",
      "7  52.379109  4.889793       6.0   \n",
      "\n",
      "                                             website  \n",
      "1  http://www.hampshire-hotels.com/amsterdam-amer...  \n",
      "4                http://www.citycenter-amsterdam.com  \n",
      "5                     http://www.hotelbluetower.com/  \n",
      "6                       http://www.hotelespresso.nl/  \n",
      "7                     http://www.hotelsebastians.nl/  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import ast\n",
    "\n",
    "def combine_csv_files(csv_filenames, pickle_filename):\n",
    "    \"\"\"\n",
    "    Combines multiple CSV files into a single Pandas DataFrame, removes rows with empty 'reviews_text',\n",
    "    and saves the cleaned DataFrame as a pickle file.\n",
    "\n",
    "    :param csv_filenames: List of CSV file paths to be combined.\n",
    "    :param pickle_filename: The name of the output pickle file.\n",
    "    :return: Cleaned Pandas DataFrame.\n",
    "    \"\"\"\n",
    "    data_frames = []\n",
    "\n",
    "    for file in csv_filenames:\n",
    "        if not os.path.isfile(file):\n",
    "            print(f\"Warning: {file} does not exist and will be skipped.\")\n",
    "            continue\n",
    "        try:\n",
    "            # Add low_memory=False to avoid the DtypeWarning\n",
    "            df = pd.read_csv(file, low_memory=False)\n",
    "            print(f\"Successfully read {file} with {len(df)} records.\")\n",
    "            data_frames.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file}: {e}\")\n",
    "\n",
    "    if not data_frames:\n",
    "        raise ValueError(\"No valid CSV files were provided.\")\n",
    "\n",
    "    # Concatenate all DataFrames into one\n",
    "    combined_df = pd.concat(data_frames, ignore_index=True)\n",
    "    print(f\"Combined DataFrame has {len(combined_df)} records and {len(combined_df.columns)} columns.\")\n",
    "\n",
    "    # Optional: Parse complex fields from strings to Python objects\n",
    "    for column in ['reviews_text', 'details_text']:\n",
    "        if column in combined_df.columns:\n",
    "            combined_df[column] = combined_df[column].apply(parse_complex_field)\n",
    "            print(f\"Parsed column: {column}\")\n",
    "\n",
    "    # --------------------- New Code Starts Here --------------------- #\n",
    "    # Remove all rows where 'reviews_text' is empty or NaN\n",
    "    if 'reviews_text' in combined_df.columns:\n",
    "        initial_count = len(combined_df)\n",
    "        \n",
    "        # Remove rows where 'reviews_text' is NaN\n",
    "        combined_df = combined_df.dropna(subset=['reviews_text'])\n",
    "        \n",
    "        # Define a function to check if 'reviews_text' is empty or contains only empty strings\n",
    "        def is_reviews_text_empty(reviews):\n",
    "            if isinstance(reviews, list):\n",
    "                # Check if the list is empty\n",
    "                if not reviews:\n",
    "                    return True\n",
    "                # Check if all elements in the list are empty strings or contain only whitespace\n",
    "                return all(isinstance(review, str) and review.strip() == '' for review in reviews)\n",
    "            # If not a list, consider it non-empty (or handle accordingly)\n",
    "            return False\n",
    "        \n",
    "        # Apply the function to identify empty 'reviews_text'\n",
    "        empty_reviews_mask = combined_df['reviews_text'].apply(is_reviews_text_empty)\n",
    "        num_empty = empty_reviews_mask.sum()\n",
    "        \n",
    "        # Remove rows where 'reviews_text' is empty\n",
    "        combined_df = combined_df[~empty_reviews_mask]\n",
    "        \n",
    "        final_count = len(combined_df)\n",
    "        removed_rows = initial_count - final_count\n",
    "        print(f\"Removed {removed_rows} rows with empty 'reviews_text'. Remaining records: {final_count}\")\n",
    "    else:\n",
    "        print(\"Column 'reviews_text' does not exist. No rows removed based on 'reviews_text'.\")\n",
    "    # --------------------- New Code Ends Here --------------------- #\n",
    "\n",
    "    # Save the cleaned DataFrame as a pickle file\n",
    "    try:\n",
    "        with open(pickle_filename, 'wb') as pkl_file:\n",
    "            pickle.dump(combined_df, pkl_file)\n",
    "        print(f\"Data successfully saved to {pickle_filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving pickle file: {e}\")\n",
    "        raise e\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "def parse_complex_field(field):\n",
    "    \"\"\"\n",
    "    Parses a string representation of a list or dictionary into actual Python objects.\n",
    "\n",
    "    :param field: String representation of the field.\n",
    "    :return: Parsed Python object (list or dict) or the original value if parsing fails.\n",
    "    \"\"\"\n",
    "    if pd.isnull(field) or field == '':\n",
    "        return None\n",
    "    try:\n",
    "        # Attempt to parse the field using ast.literal_eval\n",
    "        parsed_field = ast.literal_eval(field)\n",
    "        return parsed_field\n",
    "    except (ValueError, SyntaxError):\n",
    "        # If parsing fails, return the original string\n",
    "        return field\n",
    "\n",
    "def load_collection_data_frames(filename='collection_data_frames.pkl'):\n",
    "    \"\"\"\n",
    "    Loads the pickled DataFrame from the specified file.\n",
    "\n",
    "    :param filename: Path to the pickle file.\n",
    "    :return: Loaded Pandas DataFrame.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filename, 'rb') as file:\n",
    "            data_frames = pickle.load(file)\n",
    "        print(f\"Data successfully loaded from {filename}\")\n",
    "        return data_frames\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading pickle file: {e}\")\n",
    "        raise e\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define the list of CSV filenames\n",
    "    csv_files = [\n",
    "        'Translated/amsterdam_translated.csv',\n",
    "        'Translated/barcelona_translated.csv',\n",
    "        'Translated/dubai_translated.csv',\n",
    "        'Translated/paris_translated.csv',\n",
    "        'Translated/rome_translated.csv'\n",
    "    ]\n",
    "\n",
    "    # Define the output pickle file name\n",
    "    output_pickle = 'location_data_frames.pkl'\n",
    "\n",
    "    # Combine CSV files and save as pickle\n",
    "    location_data_frames = combine_csv_files(csv_files, output_pickle)\n",
    "\n",
    "    # Optional: Display the first few rows of the combined DataFrame\n",
    "    print(\"\\nFirst 5 records of the combined DataFrame:\")\n",
    "    print(location_data_frames.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully loaded from location_data_frames.pkl\n",
      "\n",
      "Search Results 2:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0d/3y0gvcjx1j74wcywdv3rd8n80000gn/T/ipykernel_10956/637137770.py:154: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  filtered_df = df[mask]\n"
     ]
    },
    {
     "ename": "IndexingError",
     "evalue": "Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexingError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m/Users/rithvikprakki/CDC_2024/search_data.ipynb Cell 5\u001b[0m line \u001b[0;36m2\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/rithvikprakki/CDC_2024/search_data.ipynb#W3sZmlsZQ%3D%3D?line=190'>191</a>\u001b[0m \u001b[39m# search_criteria_3 = {\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/rithvikprakki/CDC_2024/search_data.ipynb#W3sZmlsZQ%3D%3D?line=191'>192</a>\u001b[0m \u001b[39m#     'category': 'Accommodation',\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/rithvikprakki/CDC_2024/search_data.ipynb#W3sZmlsZQ%3D%3D?line=192'>193</a>\u001b[0m \u001b[39m#     'polarity': 8.0,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/rithvikprakki/CDC_2024/search_data.ipynb#W3sZmlsZQ%3D%3D?line=204'>205</a>\u001b[0m \u001b[39m# result_1 = search_database(df, search_criteria_1)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/rithvikprakki/CDC_2024/search_data.ipynb#W3sZmlsZQ%3D%3D?line=205'>206</a>\u001b[0m \u001b[39m# print(result_1)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/rithvikprakki/CDC_2024/search_data.ipynb#W3sZmlsZQ%3D%3D?line=207'>208</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mSearch Results 2:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/rithvikprakki/CDC_2024/search_data.ipynb#W3sZmlsZQ%3D%3D?line=208'>209</a>\u001b[0m result_2 \u001b[39m=\u001b[39m search_database(df, search_criteria_2)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/rithvikprakki/CDC_2024/search_data.ipynb#W3sZmlsZQ%3D%3D?line=209'>210</a>\u001b[0m \u001b[39mprint\u001b[39m(result_2)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/rithvikprakki/CDC_2024/search_data.ipynb#W3sZmlsZQ%3D%3D?line=211'>212</a>\u001b[0m \u001b[39m# print(\"\\nSearch Results 3:\")\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/rithvikprakki/CDC_2024/search_data.ipynb#W3sZmlsZQ%3D%3D?line=212'>213</a>\u001b[0m \u001b[39m# result_3 = search_database(df, search_criteria_3)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/rithvikprakki/CDC_2024/search_data.ipynb#W3sZmlsZQ%3D%3D?line=213'>214</a>\u001b[0m \u001b[39m# print(result_3)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/rithvikprakki/CDC_2024/search_data.ipynb#W3sZmlsZQ%3D%3D?line=218'>219</a>\u001b[0m \n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/rithvikprakki/CDC_2024/search_data.ipynb#W3sZmlsZQ%3D%3D?line=219'>220</a>\u001b[0m \u001b[39m# Example Usage of Loading Function\u001b[39;00m\n",
      "\u001b[1;32m/Users/rithvikprakki/CDC_2024/search_data.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/rithvikprakki/CDC_2024/search_data.ipynb#W3sZmlsZQ%3D%3D?line=150'>151</a>\u001b[0m         mask \u001b[39m&\u001b[39m\u001b[39m=\u001b[39m df[key] \u001b[39m==\u001b[39m value\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/rithvikprakki/CDC_2024/search_data.ipynb#W3sZmlsZQ%3D%3D?line=152'>153</a>\u001b[0m \u001b[39m# Apply the mask to filter the DataFrame\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/rithvikprakki/CDC_2024/search_data.ipynb#W3sZmlsZQ%3D%3D?line=153'>154</a>\u001b[0m filtered_df \u001b[39m=\u001b[39m df[mask]\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/rithvikprakki/CDC_2024/search_data.ipynb#W3sZmlsZQ%3D%3D?line=155'>156</a>\u001b[0m \u001b[39m# Handle the 'columns' key\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/rithvikprakki/CDC_2024/search_data.ipynb#W3sZmlsZQ%3D%3D?line=156'>157</a>\u001b[0m columns_to_return \u001b[39m=\u001b[39m search_criteria\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mall\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/CDC_2024/cdc/lib/python3.12/site-packages/pandas/core/frame.py:4093\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4091\u001b[0m \u001b[39m# Do we have a (boolean) 1d indexer?\u001b[39;00m\n\u001b[1;32m   4092\u001b[0m \u001b[39mif\u001b[39;00m com\u001b[39m.\u001b[39mis_bool_indexer(key):\n\u001b[0;32m-> 4093\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_bool_array(key)\n\u001b[1;32m   4095\u001b[0m \u001b[39m# We are left with two options: a single key, and a collection of keys,\u001b[39;00m\n\u001b[1;32m   4096\u001b[0m \u001b[39m# We interpret tuples as collections only for non-MultiIndex\u001b[39;00m\n\u001b[1;32m   4097\u001b[0m is_single_key \u001b[39m=\u001b[39m \u001b[39misinstance\u001b[39m(key, \u001b[39mtuple\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m is_list_like(key)\n",
      "File \u001b[0;32m~/CDC_2024/cdc/lib/python3.12/site-packages/pandas/core/frame.py:4149\u001b[0m, in \u001b[0;36mDataFrame._getitem_bool_array\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4143\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   4144\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mItem wrong length \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(key)\u001b[39m}\u001b[39;00m\u001b[39m instead of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   4145\u001b[0m     )\n\u001b[1;32m   4147\u001b[0m \u001b[39m# check_bool_indexer will throw exception if Series key cannot\u001b[39;00m\n\u001b[1;32m   4148\u001b[0m \u001b[39m# be reindexed to match DataFrame rows\u001b[39;00m\n\u001b[0;32m-> 4149\u001b[0m key \u001b[39m=\u001b[39m check_bool_indexer(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex, key)\n\u001b[1;32m   4151\u001b[0m \u001b[39mif\u001b[39;00m key\u001b[39m.\u001b[39mall():\n\u001b[1;32m   4152\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy(deep\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/CDC_2024/cdc/lib/python3.12/site-packages/pandas/core/indexing.py:2662\u001b[0m, in \u001b[0;36mcheck_bool_indexer\u001b[0;34m(index, key)\u001b[0m\n\u001b[1;32m   2660\u001b[0m indexer \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39mget_indexer_for(index)\n\u001b[1;32m   2661\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m \u001b[39min\u001b[39;00m indexer:\n\u001b[0;32m-> 2662\u001b[0m     \u001b[39mraise\u001b[39;00m IndexingError(\n\u001b[1;32m   2663\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUnalignable boolean Series provided as \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2664\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mindexer (index of the boolean Series and of \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2665\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mthe indexed object do not match).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2666\u001b[0m     )\n\u001b[1;32m   2668\u001b[0m result \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39mtake(indexer)\n\u001b[1;32m   2670\u001b[0m \u001b[39m# fall through for boolean\u001b[39;00m\n",
      "\u001b[0;31mIndexingError\u001b[0m: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match)."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "def load_collection_data_frames(filename='collection_data_frames.pkl'):\n",
    "    \"\"\"\n",
    "    Loads the pickled DataFrame from the specified file.\n",
    "\n",
    "    :param filename: Path to the pickle file.\n",
    "    :return: Loaded Pandas DataFrame.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filename, 'rb') as file:\n",
    "            data_frames = pickle.load(file)\n",
    "        print(f\"Data successfully loaded from {filename}\")\n",
    "        return data_frames\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading pickle file: {e}\")\n",
    "        raise e\n",
    "\n",
    "def parse_condition(cond_str, column):\n",
    "    \"\"\"\n",
    "    Parses a condition string and returns a lambda function that applies the condition to a DataFrame column.\n",
    "\n",
    "    Supported condition formats:\n",
    "    - \"x<value\"\n",
    "    - \"x<=value\"\n",
    "    - \"x>value\"\n",
    "    - \"x>=value\"\n",
    "    - \"x==value\"\n",
    "    - \"x!=value\"\n",
    "    - \"value1<x<value2\"\n",
    "    - \"value1<=x<=value2\"\n",
    "    - \"value1<x<=value2\"\n",
    "    - \"value1<=x<value2\"\n",
    "\n",
    "    :param cond_str: Condition string (e.g., \"0<x<50.4\", \"x<25\").\n",
    "    :param column: Column name to apply the condition on.\n",
    "    :return: A Pandas Series representing the condition mask.\n",
    "    \"\"\"\n",
    "    # Remove any spaces\n",
    "    cond_str = cond_str.replace(\" \", \"\")\n",
    "    \n",
    "    # Patterns for different condition types\n",
    "    pattern_double = re.compile(r'^([<>]=?)(x)([<>]=?)([\\d\\.]+)$')\n",
    "    pattern_between = re.compile(r'^([\\d\\.]+)([<>]=?)(x)([<>]=?)([\\d\\.]+)$')\n",
    "    pattern_single = re.compile(r'^(x)([<>]=?)([\\d\\.]+)$')\n",
    "    pattern_single_reverse = re.compile(r'^([\\d\\.]+)([<>]=?)(x)$')\n",
    "    \n",
    "    # Initialize mask as None\n",
    "    mask = None\n",
    "    \n",
    "    # Check for between conditions like \"0<x<50.4\" or \"10<=x<=20\"\n",
    "    match_between = re.match(r'^([\\d\\.]+)([<>]=?)(x)([<>]=?)([\\d\\.]+)$', cond_str)\n",
    "    if match_between:\n",
    "        val1, op1, _, op2, val2 = match_between.groups()\n",
    "        val1 = float(val1)\n",
    "        val2 = float(val2)\n",
    "        if op1 in ['<', '<=']:\n",
    "            condition1 = f\"{column} {op1} {val2}\"\n",
    "        else:\n",
    "            condition1 = f\"{column} {op1} {val1}\"\n",
    "        \n",
    "        if op2 in ['<', '<=']:\n",
    "            condition2 = f\"{column} {op2} {val2}\"\n",
    "        else:\n",
    "            condition2 = f\"{column} {op2} {val1}\"\n",
    "        \n",
    "        # Apply both conditions\n",
    "        mask = (df[column] > val1) if 'x>' in cond_str else (df[column] >= val1) if 'x>=' in cond_str else (df[column] < val2) if 'x<' in cond_str else (df[column] <= val2)\n",
    "        # Alternatively, apply both conditions\n",
    "        mask = (df[column] > val1) & (df[column] < val2)\n",
    "        return mask\n",
    "    \n",
    "    # Check for single conditions like \"x<25\", \"x>=10\"\n",
    "    match_single = re.match(r'^(x)([<>]=?)([\\d\\.]+)$', cond_str)\n",
    "    if match_single:\n",
    "        _, operator, value = match_single.groups()\n",
    "        value = float(value)\n",
    "        if operator == '<':\n",
    "            mask = df[column] < value\n",
    "        elif operator == '<=':\n",
    "            mask = df[column] <= value\n",
    "        elif operator == '>':\n",
    "            mask = df[column] > value\n",
    "        elif operator == '>=':\n",
    "            mask = df[column] >= value\n",
    "        elif operator == '==':\n",
    "            mask = df[column] == value\n",
    "        elif operator == '!=':\n",
    "            mask = df[column] != value\n",
    "        return mask\n",
    "    \n",
    "    # Check for reversed single conditions like \"10<x\"\n",
    "    match_single_rev = re.match(r'^([\\d\\.]+)([<>]=?)(x)$', cond_str)\n",
    "    if match_single_rev:\n",
    "        value, operator, _ = match_single_rev.groups()\n",
    "        value = float(value)\n",
    "        if operator == '<':\n",
    "            mask = df[column] > value\n",
    "        elif operator == '<=':\n",
    "            mask = df[column] >= value\n",
    "        elif operator == '>':\n",
    "            mask = df[column] < value\n",
    "        elif operator == '>=':\n",
    "            mask = df[column] <= value\n",
    "        elif operator == '==':\n",
    "            mask = df[column] == value\n",
    "        elif operator == '!=':\n",
    "            mask = df[column] != value\n",
    "        return mask\n",
    "    \n",
    "    # If no pattern matches, raise an error\n",
    "    raise ValueError(f\"Unsupported condition format: {cond_str}\")\n",
    "\n",
    "def search_database(df, search_criteria):\n",
    "    \"\"\"\n",
    "    Searches the DataFrame based on the provided search criteria.\n",
    "\n",
    "    :param df: Pandas DataFrame to search.\n",
    "    :param search_criteria: Dictionary containing search conditions and columns to return.\n",
    "                            Example:\n",
    "                            {\n",
    "                                'location': 'Amsterdam',\n",
    "                                'category': 'Accommodation',\n",
    "                                'lat': '0<x<50.4',\n",
    "                                'lng': 'x<25',\n",
    "                                'columns': 'all'\n",
    "                            }\n",
    "    :return: Filtered Pandas DataFrame based on the search criteria.\n",
    "    \"\"\"\n",
    "    mask = pd.Series([True] * len(df))\n",
    "    \n",
    "    # Iterate through each key in the search criteria\n",
    "    for key, value in search_criteria.items():\n",
    "        if key == 'columns':\n",
    "            continue  # Handle columns later\n",
    "        if key not in df.columns:\n",
    "            print(f\"Warning: Column '{key}' does not exist in the DataFrame and will be ignored.\")\n",
    "            continue\n",
    "        \n",
    "        if isinstance(value, str) and 'x' in value:\n",
    "            try:\n",
    "                condition_mask = parse_condition(value, key)\n",
    "                mask &= condition_mask\n",
    "            except ValueError as ve:\n",
    "                print(f\"Error parsing condition for column '{key}': {ve}\")\n",
    "                continue\n",
    "        else:\n",
    "            # Exact match\n",
    "            mask &= df[key] == value\n",
    "    \n",
    "    # Apply the mask to filter the DataFrame\n",
    "    filtered_df = df[mask]\n",
    "    \n",
    "    # Handle the 'columns' key\n",
    "    columns_to_return = search_criteria.get('columns', 'all')\n",
    "    if columns_to_return != 'all':\n",
    "        if isinstance(columns_to_return, list):\n",
    "            # Validate columns\n",
    "            valid_columns = [col for col in columns_to_return if col in df.columns]\n",
    "            invalid_columns = set(columns_to_return) - set(valid_columns)\n",
    "            if invalid_columns:\n",
    "                print(f\"Warning: The following columns are invalid and will be ignored: {invalid_columns}\")\n",
    "            if valid_columns:\n",
    "                filtered_df = filtered_df[valid_columns]\n",
    "            else:\n",
    "                print(\"Warning: No valid columns specified. Returning all columns.\")\n",
    "        else:\n",
    "            print(\"Warning: 'columns' should be a list or 'all'. Returning all columns.\")\n",
    "    \n",
    "    return filtered_df\n",
    "\n",
    "# Load the DataFrame from the pickle file\n",
    "df = load_collection_data_frames('location_data_frames.pkl')\n",
    "\n",
    "# Example Search Criteria\n",
    "# search_criteria_1 = {\n",
    "#     'location': 'Amsterdam',\n",
    "#     'category': 'accommodation',\n",
    "#     'lat': '50<x<53',\n",
    "#     'lng': '4<x<5',\n",
    "#     'columns': 'all'\n",
    "# }\n",
    "\n",
    "search_criteria_2 = {\n",
    "    'location': 'Barcelona',\n",
    "    'columns': ['place_name', 'address', 'website']\n",
    "}\n",
    "\n",
    "# search_criteria_3 = {\n",
    "#     'category': 'Accommodation',\n",
    "#     'polarity': 8.0,\n",
    "#     'columns': ['place_name', 'polarity']\n",
    "# }\n",
    "\n",
    "# search_criteria_4 = {\n",
    "#     'lat': '52.0<x<53.0',\n",
    "#     'lng': '4.8<x<4.9',\n",
    "#     'columns': ['place_name', 'lat', 'lng']\n",
    "# }\n",
    "\n",
    "# # Perform Searches\n",
    "# print(\"\\nSearch Results 1:\")\n",
    "# result_1 = search_database(df, search_criteria_1)\n",
    "# print(result_1)\n",
    "\n",
    "print(\"\\nSearch Results 2:\")\n",
    "result_2 = search_database(df, search_criteria_2)\n",
    "print(result_2)\n",
    "\n",
    "# print(\"\\nSearch Results 3:\")\n",
    "# result_3 = search_database(df, search_criteria_3)\n",
    "# print(result_3)\n",
    "\n",
    "# print(\"\\nSearch Results 4:\")\n",
    "# result_4 = search_database(df, search_criteria_4)\n",
    "# print(result_4)\n",
    "\n",
    "# Example Usage of Loading Function\n",
    "if __name__ == \"__main__\":\n",
    "    # You can place the search examples here or elsewhere as needed\n",
    "    pass  # Already executed above\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cdc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
